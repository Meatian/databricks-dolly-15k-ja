{"index":"2928","instruction":"Large言語モデルに関するこれらの段落を踏まえて、幻覚とは何か？","input":"大規模言語モデル（LLM）とは、自己教師あり学習を利用して大量のラベルなしテキストで学習させた、多くのパラメータ（通常数十億以上の重み）を持つニューラルネットワークからなる言語モデルである。LLMは2018年頃に登場し、さまざまなタスクで優れた性能を発揮しています。これにより、自然言語処理研究の焦点は、特定のタスクに特化した教師ありモデルを訓練するという以前のパラダイムからシフトしています。\n\n物件紹介\n大規模言語モデルという用語に正式な定義はありませんが、数十億以上のパラメータ数を持つディープラーニングモデルを指すことが多いです。LLMは、ある特定のタスク（感情分析、名前付きエンティティ認識、数学的推論など）のために訓練されたものとは異なり、幅広いタスクに秀でた汎用的なモデルです。LLMがタスクを達成するスキルや、LLMが可能なタスクの範囲は、設計のブレークスルーに依存しない方法で、LLMに費やされるリソース（データ、パラメータサイズ、計算能力）の量の関数であるようです。\n\n文中の次の単語を予測するという単純なタスクで訓練されたニューラル言語モデルは、十分な訓練とパラメータ数により、人間の言語の構文と意味論の多くを捉えることができることがわかった。さらに、大規模な言語モデルは、世界に関するかなりの一般的知識を示し、訓練中に大量の事実を「記憶」することができる。\n\n幻覚\n主な記事幻覚(人工知能)\n人工知能全般、特に大規模言語モデルにおいて、「幻覚」とは、モデルの学習データでは正当化されないと思われる自信に満ちた応答のことです。\n\nエマージェンシー能力\n\n質問応答などのタスクを含む多くの自然言語ベンチマークにおいて、モデルはある規模（この場合、学習計算量によって測定される）に達するまでは偶然の産物よりも性能が劣り、その時点で性能が急激に向上することが分かっています。これが「創発的能力」の例です。\n大規模な言語モデルで観測された、より単純なモデルにはなかった（モデルに明示的に設計されていない）予測不可能な能力は、通常「創発的能力」と呼ばれます。研究者は、このような能力は「より小さなモデルの性能を外挿するだけでは予測できない」と指摘する。これらの能力は、プログラムされたり設計されたりするのではなく、発見されるもので、場合によっては、LLMが公に展開された後に初めて発見されることもある。何百もの創発的な能力が報告されています。例えば、多段階算数、大学レベルの試験、単語の意図する意味の特定、思考の連鎖の促し、国際音声アルファベットの解読、単語の文字のスクランブル解除、ヒングリッシュ（ヒンディー語と英語の組み合わせ）のパラグラフの不快な内容の特定、キスワヒリの諺の類似英語版の生成などです。\n\nアーキテクチャとトレーニング\n大規模言語モデルは、2018年以降、逐次データの標準的な深層学習手法となったトランスフォーマーアーキテクチャが最もよく使われています（以前はLSTMのようなリカレントアーキテクチャが最も一般的でした）。LLMは、注釈のないテキストに対して教師なし方式で学習されます。左から右への変換器が訓練され、前の文脈を考慮した上で、訓練データ中の次の単語に割り当てられる確率を最大化する。また、LLMは（BERTの例のように）双方向変換器を使用することもでき、前後の文脈にアクセスすることで単語に対する確率分布を割り当てることができる。次の単語を予測するタスクや「空白を埋める」タスクに加えて、LLMはデータ分布の理解をテストする補助的なタスクで訓練されることもある。例えば、次文予測（NSP）では、文のペアが提示され、モデルはそれらが訓練コーパスで並んで表示されているかどうかを予測しなければならない。\n\n初期のLLMは、数十億語のオーダーを持つコーパスで訓練されました。OpenAIのGPTシリーズの最初のモデルは、2018年に9億8500万語からなるBookCorpusで訓練されました。同年、BERTはBookCorpusと英語版Wikipediaの組み合わせで訓練され、合計で33億語になりました。それ以来、LLMのトレーニングコーパスは桁違いに増え、最大で数千億、数兆のトークンに達しています。\n\nLLMは訓練に計算コストがかかる。2020年の研究では、15億パラメータのモデル（当時の最先端技術より1～2桁小さい）を訓練するコストを160万ドルと見積もっています。\n\n2020年の解析では、ニューラル言語モデルの能力（学習損失で測定）は、パラメータ数、学習データ量、学習に使用する計算量とべき乗の関係で滑らかに増加することがわかった[11][12] これらの関係は、広い範囲の値（最大7桁まで）でテストされ、範囲の最高端では関係の減衰は見られなかった（パラメータの兆個までのネットワークサイズを含む）。\n\n下流作業への応用\n2018年から2020年にかけて、特定の自然言語処理（NLP）タスクにLLMを利用するための標準的な方法は、タスクに特化した追加トレーニングでモデルを微調整することでした。GPT-3のようなより強力なLLMは、解決すべき問題をテキストプロンプトとしてモデルに提示し、場合によっては類似の問題とその解決策のテキスト例をいくつか提示する「プロンプティング」技術により、追加トレーニングなしでタスクを解決できることがその後判明しました。\n\nファインチューニング\n主な記事ファインチューニング（機械学習）\nファインチューニングとは、特定のタスク（例：感情分析、名前付きエンティティ認識、品詞タグ付け）に対して（教師ありの方法で）訓練することで、事前に訓練した既存の言語モデルを修正することである。伝達学習の一種である。一般に、言語モデルの最終層と下流タスクの出力をつなぐ新しい重みのセットを導入する必要がある。言語モデルの元の重みは「凍結」され、出力に接続する新しい重みのレイヤーのみがトレーニング中に学習されるようになる。あるいは、元の重みは小さな更新を受けることもある（以前の層が凍結されることもある）。\n\nプロンプティング\nこちらもご参照ください：プロンプトエンジニアリング、数撃ちゃ当たる学習（自然言語処理）\nGPT-3に代表されるプロンプトパラダイムでは、解決すべき問題がテキストプロンプトで示され、モデルはそれを（推論によって）補完することによって解決しなければならない。数発プロンプト」では、プロンプトには類似した（問題、解決）ペアの少数の例が含まれる。例えば、映画レビューの感情をラベル付けする感情分析タスクは、次のようにプロンプトされます：\n\nレビューです：この映画、臭い。\nセンチメント：ネガティブ\n\nレビューです：この映画は素晴らしいです！\n感傷に浸る：\n\nモデルが「正」を出力すれば、課題を正しく解決したことになる。ゼロショットプロンプトでは、解答例は提供されない。同じセンチメント分析タスクのゼロショットプロンプトの例は、「映画レビュー『この映画は素晴らしい！』に関連するセンチメントは」である。\n\nLLMの数発の性能は、NLPタスクにおいて競争力のある結果を達成することが示されており、時には先行する最先端の微調整アプローチを凌ぐこともあります。このようなNLPタスクの例としては、翻訳、質問応答、クローズタスク、単語のスクランブル解除、文中に新しい単語を使用するなどがある。このようなプロンプトを作成し、最適化することをプロンプトエンジニアリングと呼ぶ。\n\nインストラクションチューニング\nインストラクションチューニングは、より自然で正確なゼロショットプロンプトのインタラクションを促進するために設計されたファインチューニングの一形態である。テキスト入力があると、事前に訓練された言語モデルは、訓練されたテキストの分布と一致する補完を生成する。ハムレットの主要テーマについてエッセイを書きなさい」というプロンプトを与えられたナイーブな言語モデルは、「3月17日以降に受け取った提出物には、1日あたり10％の遅延ペナルティが適用されます」というような補完を提供するかもしれません。インストラクション・チューニングでは、自然言語による指示として定式化された多くのタスクの例と適切な応答で言語モデルを学習させる。インストラクションチューニングのための様々な手法が実際に適用されています。OpenAIのInstructGPTプロトコルでは、人間が生成した（プロンプト、レスポンス）ペアのデータセットに対して教師付き微調整を行い、その後、人間の嗜好のデータセットに基づいて報酬関数を学習するRLHF（reinforcement learning from human feedback）を行っています。また、「セルフインストラクト」と呼ばれる手法では、LLMが生成した例文（人間が生成した小さな初期例文からブートストラップしたもの）のトレーニングセットで言語モデルの微調整を行います。\n\nhttps:\/\/en.wikipedia.org\/wiki\/Large_language_model","output":"人工知能全般、特に大規模言語モデルにおいて、「幻覚」とは、モデルの学習データでは正当化されないと思われる自信に満ちた応答のことです。","category":"closed_qa"}
