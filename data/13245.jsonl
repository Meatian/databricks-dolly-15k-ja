{"index":"13245","instruction":"スチュアート・ラッセル著「ヒューマン・コンパチブル」の以下の要約を踏まえて、AI開発の標準的なアプローチの限界を克服するために、著者が提案することは何か。","input":"ラッセルはまず、AI研究の標準的なモデル、すなわち、人間が指定した厳格な目標を達成するために、より良くなっていくことを成功の主要な定義としているが、これは危険なほど見当違いである、と主張する。目標に含まれない人間の価値観が考慮されていないなど、目標が人間の設計者の意図を反映していない可能性がある。仮に標準モデルで開発されたAIが超知能化した場合、人間の価値観を十分に反映できない可能性が高く、人類に壊滅的な打撃を与える可能性がある。ラッセルは、人間レベルのAIや超知能AIの開発スケジュールが非常に不透明であるからこそ、一刻も早く安全性の研究を開始すべきであると主張する。\n\nラッセルは、経済的な圧力から、AI能力の継続的な進歩は避けられないと主張している。こうした圧力は、自動運転車やパーソナル・アシスタント・ソフトウェアなど、既存のAI技術の開発にすでに見られる。さらに、人間レベルのAIは、何兆ドルもの価値を持つ可能性があるという。次にラッセルは、AIリスクをめぐる現在の議論について検証する。ラッセルは、AIリスクを否定する一般的な議論に反論し、その多くは、AI研究者がAIリスクへの懸念を自分たちの分野への「攻撃」と見なすような部族主義に起因していると指摘します。ラッセルは、AIリスクに関する懸念を真剣に受け止める正当な理由があり、経済的な圧力によってAIの継続的な革新が不可避であることを繰り返し述べています。\n\nそして、ラッセルは、人間への敬意を重視した、証明可能な有益なマシンを開発するためのアプローチを提案する。AIの標準的なモデルでは、目的は厳格で確実なものであるが、このアプローチでは、AIの真の目的は不確実なままであり、AIは人間や世界についての情報を得るにつれて、その目的について確実なものに近づいていくことになる。この不確実性によって、人間の好みに対する破滅的な誤解を防ぎ、人間との協力やコミュニケーションを促進することが理想である。ラッセルは最後に、AIの研究開発に対するガバナンスを強化し、AIが支配する世界で保持すべき適切な自律性の量について文化的内省を行うよう呼びかけている。","output":"ラッセルは、人間にとって何が本当に有益なのか、徐々に知識を身につけさせるような方法でAIを訓練するべきだと提案する。そうして開発されたAIは、人間に十分な自律性を残しつつ、人間と協力することになる。","category":"closed_qa"}
