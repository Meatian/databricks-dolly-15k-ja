{"index":"13736","instruction":"畳み込みニューラルネットワークの主要な構成要素\/アーキテクチャを簡潔に箇条書きにしたものを教えてください。","input":"畳み込みニューラルネットワークは、入力層、隠れ層、出力層から構成されています。フィードフォワード・ニューラルネットワークでは、中間層は活性化関数と最終的な畳み込みによって入力と出力が隠蔽されるため、隠れ層と呼ばれることがある。畳み込みニューラルネットワークでは、隠れ層には畳み込みを行う層が含まれます。典型的には、畳み込みカーネルとその層の入力行列とのドット積を実行する層が含まれます。この積は通常フロベニウス内積であり、活性化関数は一般にReLUである。畳み込みカーネルがその層の入力行列に沿ってスライドすると、畳み込み演算によって特徴マップが生成され、それが次の層の入力に寄与する。この後、プーリング層、完全連結層、正規化層などの層が続く。\n\n畳み込み層\n\n畳み込み層は、入力を畳み込み、その結果を次の層に渡します。これは、視覚野のニューロンが特定の刺激に反応するのと似ている。各畳み込みニューロンは、その受容野に対してのみデータを処理する。完全連結フィードフォワードニューラルネットワークは、特徴の学習やデータの分類に使用することができますが、このアーキテクチャは、大きな入力（例えば、高解像度の画像）には一般的に実用的ではありません。100×100サイズの画像に対する完全連結層は、第2層の各ニューロンに10,000の重みを持たせる。畳み込みは、自由なパラメータの数を減らし、ネットワークをより深くすることを可能にします。例えば、5×5のタイリング領域を使用し、それぞれが同じ共有重みを持つ場合、必要なニューロンは25個だけです。より少ないパラメータで正則化された重みを使用することで、以前のニューラルネットワークのバックプロパゲーションで見られた勾配の消失や爆発的な勾配の問題を回避することができます。\n\n処理を高速化するために、標準的な畳み込み層を深さ方向に分離可能な畳み込み層に置き換えることができる。この畳み込み層は、深さ方向の畳み込みに続いて点方向の畳み込みを行うことに基づいている。深さ方向の畳み込みは、入力テンソルの各チャンネルに独立して適用される空間畳み込みであり、一方、点方向の畳み込みは、以下の使用に制限された標準的な畳み込みである。\n\nプーリングレイヤー\n畳み込みネットワークは、従来の畳み込み層に加えて、ローカルプーリング層やグローバルプーリング層を含むことができる。プーリング層は、ある層のニューロン・クラスターの出力を次の層の単一ニューロンに結合することで、データの次元を縮小する。ローカルプーリングは小さなクラスターを結合するもので、2×2などのタイリングサイズが一般的に使用されます。グローバルプーリングは、特徴マップのすべてのニューロンに対して作用する。一般的に使用されているプーリングには、最大と平均の2種類があります。マックスプーリングは、特徴マップの各ニューロンのローカルクラスタの最大値を使用し、アベレージプーリングは、平均値を使用します。\n\n完全連結層\n完全接続層は、ある層のすべてのニューロンを別の層のすべてのニューロンに接続します。これは従来の多層パーセプトロンニューラルネットワーク（MLP）と同じです。平坦化された行列は、完全接続層を経由して画像を分類する。\n\nレセプティブフィールド\nニューラルネットワークでは、各ニューロンは前の層のいくつかの位置から入力を受ける。畳み込み層では、各ニューロンは、ニューロンの受容野と呼ばれる前の層の限定された領域のみから入力を受ける。通常、この領域は正方形である（例えば、5×5ニューロン）。一方、完全連結層では、受容野は前の層全体となる。したがって、各畳み込み層では、各ニューロンは前の層よりも広い領域から入力を取り込むことになる。これは、畳み込みを何度も適用することで、画素の値だけでなく、その周囲の画素も考慮に入れるためです。拡張レイヤーを使用する場合、受容野のピクセル数は一定ですが、複数のレイヤーの効果を組み合わせると、その次元が大きくなるにつれて、受容野はより疎になります。\n\n受容野の大きさを任意に操作するために、標準的な畳み込み層の代わりになるものがいくつかある。例えば、アトラス畳み込みや拡張畳み込みは、可視領域とブラインド領域をインターリーブすることにより、パラメータ数を増やすことなく、受容野サイズを拡大する。さらに、1つの拡張畳み込み層は、複数の拡張比を持つフィルターで構成されるため、可変の受容野サイズを持つことができる。\n\n重量\nニューラルネットワークの各ニューロンは、前の層の受容野から受け取った入力値に特定の関数を適用することで出力値を計算する。入力値に適用される関数は、重みとバイアス（通常は実数）のベクトルによって決定される。学習は、これらのバイアスと重みを繰り返し調整することで行われる。\n\n重みとバイアスのベクトルはフィルターと呼ばれ、入力の特定の特徴（例えば、特定の形状）を表現する。CNNの特徴は、多くのニューロンが同じフィルタを共有できることです。これは、各受容野がそれぞれ独自のバイアスとベクトルの重み付けを持つのとは対照的に、そのフィルターを共有するすべての受容野で単一のバイアスと単一の重みのベクトルが使用されるため、メモリフットプリントが減少する。","output":"畳み込みニューラルネットワークの主要な構成要素\/アーキテクチャは以下の通りです：\n\n畳み込み層：入力行列を畳み込む層で、自由パラメータの数を減らし、ネットワークをより深くすることができます。\n\nプーリング層：この層は、ある層のニューロン・クラスターの出力を次の層の単一のニューロンに結合することによって、データの次元を縮小します。\n\n完全連結層：この層は、ある層のすべてのニューロンを、別の層のすべてのニューロンに接続します。\n\n受容野：各ニューロンは、ニューロンの受容野と呼ばれる前の層の制限された領域から入力を受ける。畳み込み層では、各ニューロンは前の層よりも入力の広い領域から入力を受ける。\n\n重み（Weights）：重みとバイアスのベクトルは、ニューラルネットワークが入力値から出力値を計算するために使用されます。","category":"summarization"}
