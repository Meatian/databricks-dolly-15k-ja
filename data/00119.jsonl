{"index":"119","instruction":"SVMとは？","input":"機械学習において、サポートベクターマシン（SVM、サポートベクターネットワーク[1]とも）は、分類や回帰分析のためにデータを分析する関連学習アルゴリズムを持つ教師あり学習モデルである。AT&Tベル研究所でVladimir Vapnikとその同僚によって開発された(Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,[1] Vapnik et al., 1997[citation needed]) SVMは最も堅牢な予測方法の一つで、Vapnik (1982, 1995) と Chervonenkis (1974) により提案された統計学習枠組みまたはVC理論に基づいている。SVMの学習アルゴリズムは、2つのカテゴリのいずれかに属するとマークされた訓練例の集合が与えられると、新しい例をどちらかのカテゴリに割り当てるモデルを構築し、非確率的な2値線形分類器とする（ただし、確率的分類設定でSVMを使用するPlatt Scalingなどの手法が存在する）。SVMは、2つのカテゴリー間のギャップの幅を最大化するように、学習例を空間の点にマッピングします。そして、新しい例は同じ空間にマッピングされ、ギャップのどちら側に属するかに基づいてカテゴリーに属すると予測される。\n\nSVMは線形分類だけでなく、カーネルトリックと呼ばれる、入力を高次元特徴空間に暗黙のうちにマッピングすることで、非線形分類を効率的に行うことができます。\n\nHava SiegelmannとVladimir Vapnikによって作られたサポートベクタークラスタリング[2]アルゴリズムは、サポートベクターマシンアルゴリズムで開発されたサポートベクトルの統計学を適用し、ラベルのないデータを分類します[citation needed] これらのデータセットでは、教師なし学習アプローチが必要です。これは、データのグループへの自然なクラスタリングを見つけ、これらのクラスタに従って新しいデータをマッピングしようとします。","output":"SVMとは、\"\"サポートベクターマシン\"\"の略です。教師あり学習タスクのための機械学習モデルである。ディープラーニングが流行する前の1990年代から2000年代前半にかけて流行しました。古典的な使用例は二値分類ですが、その拡張版として、マルチラベル分類や回帰も可能です。カーネル法は、SVMが非線形機械学習タスクを実行する能力を大幅に向上させました。","category":"closed_qa"}
